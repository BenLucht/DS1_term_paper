%\begin{itemize}
%\item Summarize the main points and achievements
%\item Add your own assessment/criticism on the topic
%\end{itemize}

After brief conclusions for each clustering method follows a more general conclusion discussing observations and particularities that stand out when looking at the functioning and results for all methods on all data sets with all evaluation methods.

\textit{K-Means} - Generally K-Means clustering is a fast, easy to use and reliable method to cluster data. For house pricing, seeds and mall customer data it works well at segmenting into separate partitions which can be inspected visually to be what a human would describe as distinct clusters. When the underlying structure of the data lends itself to the globular type of cluster shapes K-Means is known for like the seeds data set the result is dependable as the use of classification scores has shown. House pricing and wine quality data show that high dimensional and overlapping data poses a challenge because especially in the wine data set (barely any separation) K-Means \textit{will} find clusters of similar size and shape even if those do not represent the underlying data well.

\textit{Mean Shift} - All in all, Mean Shift algorithm is known to work best on low-dimensional datasets but nevertheless, it performed better on our high-dimensional datasets about Boston House Pricing and Redwine Quality in contrast to the low-dimensional datasets about Seeds and Mall Customer Data. As Mean Shift is density-based, this phenomena can be explained taking into account not only dataset dimensionality but also density. For the best Mean Shift performance, datasets consisting of dense regions which are distant from each other are required. 
The optimal value for the bandwidth parameter has to be evaluated for each dataset individually.

\textit{Affinity Propagation} - Overall I would rate affinity propagation as a medium cluster algorithm because it computes a good clustering but it depends on the dataset and on the parameters. Affinity Propagation works on the first and second dataset good, but on the third and fourth it works not so good. If you work with affinity propagation without fitting the parameters you don’t get a good clustering. This happened in all four datasets. Also sometimes the algorithm do not convergent. A good advantage of affinity propagation is that it don’t required the numbers of cluster. Because of this property affinity propagation is good if you don’t know that dataset much, but the calculation is long, because it depends on the datapoints in the dataset. It also works better with dataset with has not so high dimensions like the Consumers and the seeds dataset. For dataset, which have many dimensions , affinity propagation will find many cluster. \\

As a high level overview it can be said that no one single algorithm outperforms the others in all or most circumstances. Each has particular advantages which could be seen for the different data sets. Some particularities can however be noted. Using K-Means and Mean Shift can regularly produce remarkably similar clustering results for the data sets at hand despite their different method of arriving there. 

For visually reasonable clusterings \gls{CH} often sees highest values for K-Means clustering, which makes sense given its construction (see section \ref{sec:evaluation_description}. However its meaningfulness does seem to break down when there is an excessive number of clusters, possibly skewing the perception of clustering success when solely relying on this type of evaluation as can be seen in table \ref{tab:evalutaion_table}. On this topic it should be reiterated that using evaluation indices on the same algorithm for different parameter configurations might be more meaningful than comparing different algorithms when not taking additional information into account. Also relying on a single measure can lead to one sided conclusions.

A few observations on the application of the clustering algorithm implementations: Affinity Clustering seems to be somewhat unstable when trying out different preference parameter values, regularly failing to converege even when significantly increasing the iteration limit. Run time for Affinity Clustering and Spectral Clustering can be notably longer than Mean Shift and especially the utilized fast method for K-Means.

Given the opportunity to compare clustering results with original labeling for two of the data sets confirms the expectation that accurately assigning observations to groups is harder when separation is lacking. This is particularly evident in the results for wine quality data where all algorithms do find clusters but the actual data is highly dispersed and overlapping.
