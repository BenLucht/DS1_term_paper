%\begin{itemize}
%\item Summarize the main points and achievements
%\item Add your own assessment/criticism on the topic
%\end{itemize}

After brief conclusions on each clustering method, a more general conclusion follows, discussing observations and particularities that stand out when looking at the functioning and results for all methods on all data sets with all evaluation methods.

\textit{K-Means} - Generally K-Means clustering is a fast, easy to use and reliable method to cluster data. For house pricing, seeds and mall customer data it works well at segmenting into separate partitions which can be inspected visually to be what a human would describe as distinct clusters. When the underlying structure of the data lends itself to the globular type of cluster shapes K-Means is known for, like the seeds data set, the result is dependable as the use of classification scores has shown. House pricing and wine quality data show that high dimensional and overlapping data poses a challenge because especially in the wine data set (barely any separation) K-Means \textit{will} find clusters of similar size and shape even if those do not represent the underlying data well.

\textit{Mean Shift} - Normally, Mean Shift algorithm is known to work best on low-dimensional data sets but nevertheless, it performed better on our high-dimensional data sets about Boston House Pricing and Red Wine Quality in contrast to the low-dimensional data sets about Seeds and Mall Customer Data. As Mean Shift is density-based, this phenomenon can be explained taking not only data set dimensionality but also density into account. For the best Mean Shift performance, data sets consisting of dense regions which are distant from each other are required.
One main advantage is that Mean Shift neither makes any assumptions about the number of resulting clusters, nor on their shape. The only parameter that is required is the kernel's radius which may often be easier to estimate for real world data than the number of natural groups.
The optimal value for the bandwidth parameter has to be evaluated for each data set individually.

\textit{Affinity Propagation} - Overall, we would rate Affinity Propagation as a medium cluster algorithm because it can compute a good clustering, but this depends on the given data set and on the parameter values. Affinity Propagation clustering results for the first and second data set seem to be suitable, but those for the third and fourth data set do not represent plausible structures. Applying the Affinity Propagation algorithm without fitting the parameters will not result in a good clustering. This happened in all four data sets. Sometimes, the algorithm does not converge which makes it rather unstable. An advantage of Affinity Propagation is that it does not require the number of clusters beforehand. Because of this property, Affinity Propagation is good if you don’t know much about the data set, but the calculation is slowly because it depends on the data points in the data set. It also works better with data sets consisting of low dimensions like the Mall Customers and the Seeds data set. For a data set which has many dimensions, Affinity Propagation will find a lot of clusters.

\textit{Spectral Clustering} – To sum up, the Spectral Clustering algorithm is a very popular and easy implementable algorithm. It is computationally expensive for large data sets such as the Wine Quality data set. The reason for that is that eigenvalues and eigenvectors need to be computed and then clustering is being done on these vectors. For large data sets this may increase
time complexity. \newline
In general, Spectral Clustering is not making any problems with the data sets we have chosen. The main advantage of Spectral Clustering is that it is applicable for non-convex geometries and there is no assumption made about the shape or form of the clusters even intertwined spirals are possible. Only waiting for the plot to be build can put you on hold.\\

As a high level overview it can be said that no one single algorithm outperforms the others in all or most circumstances. Each has particular advantages which could be seen for the different data sets. Some particularities can however be noted. Using K-Means and Mean Shift can regularly produce remarkably similar clustering results for the data sets at hand despite their different methods of arriving there. 

For visually reasonable clusterings, \gls{CH} often sees highest values for K-Means clustering, which makes sense given its construction (see section \ref{sec:evaluation_description}). However, its meaningfulness does seem to break down when there is an excessive number of clusters, possibly skewing the perception of clustering success when solely relying on this type of evaluation as can be seen in table \ref{tab:evalutaion_table}. On this topic it should be reiterated that using evaluation indices on the same algorithm for different parameter configurations might be more meaningful than comparing different algorithms when not taking additional information into account. Also relying on a single measure can lead to one sided conclusions.

A few observations on the application of the clustering algorithm implementations: Affinity Clustering seems to be somewhat unstable when trying out different preference parameter values, regularly failing to converge even when significantly increasing the iteration limit. Run time for Affinity Clustering and Spectral Clustering can be notably longer than Mean Shift and especially the utilized fast method for K-Means.

Given the opportunity to compare clustering results with original labelling for two of the data sets confirms the expectation that accurately assigning observations to groups is harder when separation is lacking. This is particularly evident in the results for Wine Quality data where all algorithms do find clusters but the actual data is highly dispersed and overlapping.

All in all, it is highly dependent on the given data set in terms of its dimensionality and distribution of data points which algorithm should be applied to find a reasonable clustering result.