%\begin{itemize}
%\item Summarize the main points and achievements
%\item Add your own assessment/criticism on the topic
%\end{itemize}

After brief conclusions on each clustering method follows a more general conclusion discussing observations and particularities that stand out when looking at the functioning and results for all methods on all data sets with all evaluation methods.

\textit{K-Means} - Generally K-Means clustering is a fast, easy to use and reliable method to cluster data. For house pricing, seeds and mall customer data it works well at segmenting into separate partitions which can be inspected visually to be what a human would describe as distinct clusters. When the underlying structure of the data lends itself to the globular type of cluster shapes K-Means is known for, like the seeds data set, the result is dependable as the use of classification scores has shown. House pricing and wine quality data show that high dimensional and overlapping data poses a challenge because especially in the wine data set (barely any separation) K-Means \textit{will} find clusters of similar size and shape even if those do not represent the underlying data well.

\textit{Mean Shift} - All in all, Mean Shift algorithm is known to work best on low-dimensional datasets but nevertheless, it performed better on our high-dimensional datasets about Boston House Pricing and Redwine Quality in contrast to the low-dimensional datasets about Seeds and Mall Customer Data. As Mean Shift is density-based, this phenomenon can be explained taking not only dataset dimensionality but also density into account. For the best Mean Shift performance, datasets consisting of dense regions which are distant from each other are required.
The optimal value for the bandwidth parameter has to be evaluated for each dataset individually.

\textit{Affinity Propagation} - Overall, we would rate affinity propagation as a medium cluster algorithm because it can compute a good clustering, but this depends on the given dataset and on the parameter values. Affinity Propagation clustering results for the first and second dataset seem to be suitable, but those for the third and fourth dataset do not represent plausible structures. Applying the Affinity Propagation algorithm without fitting the parameters will not result in a good clustering. This happened in all four datasets. Sometimes, the algorithm does not converge which makes it rather unstable. An advantage of Affinity Propagation is that it does not require the number of clusters beforehand. Because of this property, Affinity Propagation is good if you donâ€™t know much about the dataset, but the calculation is slowly because it depends on the datapoints in the dataset. It also works better with datasets consisting of low dimensions like the Mall Customers and the Seeds dataset. For a dataset which has many dimensions, Affinity Propagation will find a lot of cluster. \\

As a high level overview it can be said that no one single algorithm outperforms the others in all or most circumstances. Each has particular advantages which could be seen for the different datasets. Some particularities can however be noted. Using K-Means and Mean Shift can regularly produce remarkably similar clustering results for the datasets at hand despite their different methods of arriving there. 

For visually reasonable clusterings, \gls{CH} often sees highest values for K-Means clustering, which makes sense given its construction (see section \ref{sec:evaluation_description}). However, its meaningfulness does seem to break down when there is an excessive number of clusters, possibly skewing the perception of clustering success when solely relying on this type of evaluation as can be seen in table \ref{tab:evalutaion_table}. On this topic it should be reiterated that using evaluation indices on the same algorithm for different parameter configurations might be more meaningful than comparing different algorithms when not taking additional information into account. Also relying on a single measure can lead to one sided conclusions.

A few observations on the application of the clustering algorithm implementations: Affinity Clustering seems to be somewhat unstable when trying out different preference parameter values, regularly failing to converge even when significantly increasing the iteration limit. Runtime for Affinity Clustering and Spectral Clustering can be notably longer than Mean Shift and especially the utilized fast method for K-Means.

Given the opportunity to compare clustering results with original labelling for two of the data sets confirms the expectation that accurately assigning observations to groups is harder when separation is lacking. This is particularly evident in the results for wine quality data where all algorithms do find clusters but the actual data is highly dispersed and overlapping.

All in all, it is highly dependent on the given dataset in terms of its dimensionality and distribution of data points which algorithm should be applied to find a reasonable clustering result.
